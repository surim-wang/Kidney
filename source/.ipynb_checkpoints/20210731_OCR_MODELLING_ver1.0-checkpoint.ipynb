{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신장의무기록사본 OCR 서비스 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 훈련용 데이터 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. 라이브러리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "os.chdir('D:/kidney/source')\n",
    "\n",
    "import tensorflow as tf\n",
    "import efficientnet.tfkeras\n",
    "import efficientnet.keras\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. 이미지 로드\n",
    "### 1-3. labelme_json 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image = cv2.imread('../image/SCAN_01.jpg')\n",
    "\n",
    "with open('../image/SCAN_01.json', \"r\", encoding='UTF8') as scan_json:\n",
    "    json_dict = json.load(scan_json)\n",
    "\n",
    "#json_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4. 이미지 크기 통일 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def border_make(image):\n",
    "    row, col = image.shape[:2]\n",
    "    bottom = image[row-2:row, 0:col]\n",
    "    mean = cv2.mean(bottom)[0]\n",
    "    \n",
    "    col_bordersize = (55-col)/2\n",
    "    row_bordersize = (55-row)/2\n",
    "    \n",
    "    border = cv2.copyMakeBorder(\n",
    "        image,\n",
    "        top = math.ceil(row_bordersize),\n",
    "        bottom = math.floor(row_bordersize),\n",
    "        left = math.ceil(col_bordersize),\n",
    "        right = math.floor(col_bordersize),\n",
    "        borderType = cv2.BORDER_ISOLATED, #BORDER_ISOLATED  BORDER_CONSTANT\n",
    "        value = [mean, mean, mean]\n",
    "    )\n",
    "    return border"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json_dict['shapes'][0]['label'] #0만 바꿔주면 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-5. Y_train의 컬럼명(=label) 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json_dict['shapes'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = []\n",
    "for i in range(len(data)):\n",
    "    label = json_dict['shapes'][i]['label']\n",
    "    Y_train.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-6. 데이터 프레임 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 컬럼명 만들기\n",
    "pixel_lst = []\n",
    "for i in range(3025): #55*55\n",
    "    name = 'pixel_{}'.format(i)\n",
    "    pixel_lst.append(name)\n",
    "\n",
    "#데이터 프레임 생성\n",
    "df= pd.DataFrame(columns= pixel_lst) # columns_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-7. 훈련용 이미지 reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image = cv2.imread('../image/SCAN_01.jpg', cv2.IMREAD_COLOR) #위에서 정의했음.\n",
    "img_lst =[]\n",
    "for i in range(len(json_dict['shapes'])):\n",
    "    # 첫 좌표\n",
    "    x1 = int(json_dict['shapes'][i]['points'][0][0]) #0,0 첫좌표의 행값\n",
    "    y1 = int(json_dict['shapes'][i]['points'][0][1]) #0,0 첫좌표의 열값\n",
    "    # 마지막 좌표\n",
    "    x2 = int(json_dict['shapes'][i]['points'][1][0])\n",
    "    y2 = int(json_dict['shapes'][i]['points'][1][1])\n",
    "    \n",
    "    #크롭 이미지\n",
    "    if x1 > x2:\n",
    "        if y1 > y2:\n",
    "            cropped_image = image[y2: y1, x2: x1].copy()    \n",
    "        else:\n",
    "            cropped_image = image[y1: y2, x2: x1].copy()    \n",
    "    else:\n",
    "        if y1 > y2:\n",
    "            cropped_image = image[y2: y1, x1: x2].copy()    \n",
    "        else:\n",
    "            cropped_image = image[y1: y2, x1: x2].copy()    \n",
    "\n",
    "    #이미지 사이즈 통일 (55*55) \n",
    "    img = border_make(cropped_image)\n",
    "    # 이미지 그레이스케일 적용\n",
    "    dst = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # 이미지 트레시 홀드 적용\n",
    "    ret ,img = cv2.threshold(dst,127,255,0)\n",
    "    \n",
    "    #SCAN_01.PNG의 모든 글자를 4차원의 형태로 만들기\n",
    "    img_lst.append(img)\n",
    "    X_train = np.array(img_lst).reshape(-1,55,55,1)    # (971, 55, 55, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.save('../data/X_train', img_array, allow_pickle=True, fix_imports=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CNN OCR 모델 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. 독립변수, 종속변수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(971, 55, 55, 1) (971, 122)\n"
     ]
    }
   ],
   "source": [
    "X_train \n",
    "Y_train = pd.get_dummies(Y_train)\n",
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1-1. 예측을 위해 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Y_train.columns)\n",
    "df.to_csv('../data/Y_train_columns.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.keras.layers.Input(shape=[55, 55, 1])\n",
    "H = tf.keras.layers.Conv2D(3, kernel_size = 5, activation = 'swish')(X)\n",
    "H = tf.keras.layers.MaxPool2D()(H)\n",
    "H = tf.keras.layers.Conv2D(6, kernel_size = 5, activation = 'swish')(H)\n",
    "H = tf.keras.layers.MaxPool2D()(H)\n",
    "H = tf.keras.layers.Flatten()(H)\n",
    "H = tf.keras.layers.Dense(84, activation = 'swish')(H)\n",
    "Y = tf.keras.layers.Dense(122, activation = 'softmax')(H)\n",
    "\n",
    "model = tf.keras.models.Model(X, Y)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. 모델 FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 971 samples\n",
      "Epoch 1/25\n",
      "971/971 [==============================] - 1s 785us/sample - loss: 22.5715 - acc: 0.0947\n",
      "Epoch 2/25\n",
      "971/971 [==============================] - 1s 817us/sample - loss: 3.7226 - acc: 0.2564\n",
      "Epoch 3/25\n",
      "971/971 [==============================] - 1s 706us/sample - loss: 2.8405 - acc: 0.4078\n",
      "Epoch 4/25\n",
      "971/971 [==============================] - 1s 734us/sample - loss: 2.1777 - acc: 0.5510\n",
      "Epoch 5/25\n",
      "971/971 [==============================] - 1s 737us/sample - loss: 1.6021 - acc: 0.6612\n",
      "Epoch 6/25\n",
      "971/971 [==============================] - 1s 740us/sample - loss: 1.3340 - acc: 0.7065\n",
      "Epoch 7/25\n",
      "971/971 [==============================] - 1s 758us/sample - loss: 1.0758 - acc: 0.7590\n",
      "Epoch 8/25\n",
      "971/971 [==============================] - 1s 776us/sample - loss: 0.9285 - acc: 0.7837\n",
      "Epoch 9/25\n",
      "971/971 [==============================] - 1s 780us/sample - loss: 0.7821 - acc: 0.8198\n",
      "Epoch 10/25\n",
      "971/971 [==============================] - 1s 762us/sample - loss: 0.6779 - acc: 0.8301\n",
      "Epoch 11/25\n",
      "971/971 [==============================] - 1s 726us/sample - loss: 0.5806 - acc: 0.8558\n",
      "Epoch 12/25\n",
      "971/971 [==============================] - 1s 729us/sample - loss: 0.5109 - acc: 0.8877\n",
      "Epoch 13/25\n",
      "971/971 [==============================] - 1s 763us/sample - loss: 0.4191 - acc: 0.8970\n",
      "Epoch 14/25\n",
      "971/971 [==============================] - 1s 736us/sample - loss: 0.4046 - acc: 0.9032\n",
      "Epoch 15/25\n",
      "971/971 [==============================] - 1s 698us/sample - loss: 0.3933 - acc: 0.9053s - loss: 0.3644 - acc\n",
      "Epoch 16/25\n",
      "971/971 [==============================] - 1s 721us/sample - loss: 0.3257 - acc: 0.9217\n",
      "Epoch 17/25\n",
      "971/971 [==============================] - 1s 717us/sample - loss: 0.3113 - acc: 0.9248\n",
      "Epoch 18/25\n",
      "971/971 [==============================] - 1s 647us/sample - loss: 0.2491 - acc: 0.9434\n",
      "Epoch 19/25\n",
      "971/971 [==============================] - 1s 697us/sample - loss: 0.2181 - acc: 0.9464\n",
      "Epoch 20/25\n",
      "971/971 [==============================] - 1s 862us/sample - loss: 0.2151 - acc: 0.9506\n",
      "Epoch 21/25\n",
      "971/971 [==============================] - 1s 735us/sample - loss: 0.1889 - acc: 0.9567\n",
      "Epoch 22/25\n",
      "971/971 [==============================] - 1s 692us/sample - loss: 0.1660 - acc: 0.9609\n",
      "Epoch 23/25\n",
      "971/971 [==============================] - 1s 756us/sample - loss: 0.1690 - acc: 0.9660\n",
      "Epoch 24/25\n",
      "971/971 [==============================] - 1s 700us/sample - loss: 0.1288 - acc: 0.9650\n",
      "Epoch 25/25\n",
      "971/971 [==============================] - 1s 727us/sample - loss: 0.1003 - acc: 0.9732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ac02dc4d48>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=25) #, steps_per_epoch = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.save('../model/OCR_ver1.0.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4. 모델 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 55, 55, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 51, 51, 3)         78        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 25, 25, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 21, 21, 6)         456       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 10, 6)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 84)                50484     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 122)               10370     \n",
      "=================================================================\n",
      "Total params: 61,388\n",
      "Trainable params: 61,388\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
