{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신장의무기록사본 OCR 서비스 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 훈련용 데이터 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. 라이브러리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "os.chdir('D:/kidney/source')\n",
    "\n",
    "import tensorflow as tf\n",
    "import efficientnet.tfkeras\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1-2-1. 이미지 크기 통일 (함수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def border_make(image):\n",
    "    row, col = image.shape[:2]\n",
    "    bottom = image[row-2:row, 0:col]\n",
    "    mean = cv2.mean(bottom)[0]\n",
    "    \n",
    "    col_bordersize = (55-col)/2\n",
    "    row_bordersize = (55-row)/2\n",
    "    \n",
    "    border = cv2.copyMakeBorder(\n",
    "        image,\n",
    "        top = math.ceil(row_bordersize),\n",
    "        bottom = math.floor(row_bordersize),\n",
    "        left = math.ceil(col_bordersize),\n",
    "        right = math.floor(col_bordersize),\n",
    "        borderType = cv2.BORDER_ISOLATED, #BORDER_ISOLATED  BORDER_CONSTANT\n",
    "        value = [mean, mean, mean]\n",
    "    )\n",
    "    return border"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1-2-2. 훈련용 이미지 reshape  (함수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeX_train(json_data, image): #json_data: 라벨미해서 나온 데이터, image: 라벨미에 사용했던 이미지\n",
    "    #image = cv2.imread('../image/SCAN_01.jpg', cv2.IMREAD_COLOR) #위에서 정의했음.\n",
    "    img_lst =[]\n",
    "    for i in range(len(json_data['shapes'])):\n",
    "        # 첫 좌표\n",
    "        x1 = int(json_data['shapes'][i]['points'][0][0]) #0,0 첫좌표의 행값\n",
    "        y1 = int(json_data['shapes'][i]['points'][0][1]) #0,0 첫좌표의 열값\n",
    "        # 마지막 좌표\n",
    "        x2 = int(json_data['shapes'][i]['points'][1][0])\n",
    "        y2 = int(json_data['shapes'][i]['points'][1][1])\n",
    "\n",
    "        #크롭 이미지\n",
    "        if x1 > x2:\n",
    "            if y1 > y2:\n",
    "                cropped_image = image[y2: y1, x2: x1].copy()    \n",
    "            else:\n",
    "                cropped_image = image[y1: y2, x2: x1].copy()    \n",
    "        else:\n",
    "            if y1 > y2:\n",
    "                cropped_image = image[y2: y1, x1: x2].copy()    \n",
    "            else:\n",
    "                cropped_image = image[y1: y2, x1: x2].copy()    \n",
    "\n",
    "        #이미지 사이즈 통일 (55*55) \n",
    "        img = border_make(cropped_image) # 함수 사용\n",
    "        # 이미지 그레이스케일 적용\n",
    "        dst = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        # 이미지 트레시 홀드 적용\n",
    "        ret ,img = cv2.threshold(dst,127,255,0)\n",
    "\n",
    "        #SCAN_01.PNG의 모든 글자를 4차원의 형태로 만들기\n",
    "        img_lst.append(img)\n",
    "        X_train = np.array(img_lst).reshape(-1,55,55,1)    # (971, 55, 55, 1)\n",
    "    return X_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1-2-3. Y_train의 컬럼명(=label) 추출 (함수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeY_train(json_data): # json_data: 라벨미해서 나온 데이터\n",
    "    # Json_shapes = json_dict['shapes']\n",
    "    Y_train = []\n",
    "    for i in range(len(json_data['shapes'])):\n",
    "        label = json_data['shapes'][i]['label']\n",
    "        Y_train.append(label)\n",
    "    return Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. 데이터 셋 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1-3-1. 이미지, labelme_json 로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_image = glob('../image/SCAN*.jpg')\n",
    "glob_json = glob('../image/*.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1-3-2. 함수 돌리기 전 리스트 만들기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "json_lst = []\n",
    "for i in range(len(glob_json)):\n",
    "    with open(glob_json[i], \"r\", encoding='UTF8') as scan_json:\n",
    "        json_data = json.load(scan_json)\n",
    "        json_lst.append(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_lst =[]\n",
    "for i in range(len(glob_image)):\n",
    "    image = cv2.imread('../image/SCAN_01.jpg')\n",
    "    image_lst.append(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4. 함수 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-10d4c4f829ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_lst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson_lst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMakeX_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMakeY_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#with open(\"../image/newfile.json\", \"w\") as new_file:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-93-07149ed9dc62>\u001b[0m in \u001b[0;36mMakeX_train\u001b[1;34m(json_data, image)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m#image = cv2.imread('../image/SCAN_01.jpg', cv2.IMREAD_COLOR) #위에서 정의했음.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mimg_lst\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'shapes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[1;31m# 첫 좌표\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'shapes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'points'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#0,0 첫좌표의 행값\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "Y_train = []\n",
    "X_train = []\n",
    "for image, json_data in zip(image_lst, json_lst):\n",
    "    x_train = MakeX_train(json_data, image)\n",
    "    y_train = MakeY_train(json_data)\n",
    "    #with open(\"../image/newfile.json\", \"w\") as new_file:\n",
    "     #   json.dump(json_lst, new_file)\n",
    "    X_train.append(x_train)\n",
    "    Y_train += y_train\n",
    "X_train = np.concatenate((X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.save('../data/X_train', img_array, allow_pickle=True, fix_imports=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CNN OCR 모델 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. 독립변수, 종속변수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train \n",
    "Y_train = pd.get_dummies(Y_train)\n",
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1-1. 예측을 위해 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Y_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.to_csv('../data/Y_train_unique.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. CNN LeNet-5 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.keras.layers.Input(shape=[55, 55, 1])\n",
    "H = tf.keras.layers.Conv2D(3, kernel_size = 5, activation = 'swish')(X)\n",
    "H = tf.keras.layers.MaxPool2D()(H)\n",
    "H = tf.keras.layers.Conv2D(6, kernel_size = 5, activation = 'swish')(H)\n",
    "H = tf.keras.layers.MaxPool2D()(H)\n",
    "H = tf.keras.layers.Flatten()(H)\n",
    "H = tf.keras.layers.Dense(84, activation = 'swish')(H)\n",
    "Y = tf.keras.layers.Dense(148, activation = 'softmax')(H)\n",
    "\n",
    "model = tf.keras.models.Model(X, Y)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. 모델 FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "When using data tensors as input to a model, you should specify the `steps_per_epoch` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-1e77d17c0035>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#, steps_per_epoch = 20)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2412\u001b[0m     \u001b[1;31m# Validates `steps` argument based on x's type.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2413\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcheck_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2414\u001b[1;33m       \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_steps_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2416\u001b[0m     \u001b[1;31m# First, we build the model on the fly if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_steps_argument\u001b[1;34m(input_data, steps, steps_name)\u001b[0m\n\u001b[0;32m   1197\u001b[0m       raise ValueError('When using {input_type} as input to a model, you should'\n\u001b[0;32m   1198\u001b[0m                        ' specify the `{steps_name}` argument.'.format(\n\u001b[1;32m-> 1199\u001b[1;33m                            input_type=input_type_str, steps_name=steps_name))\n\u001b[0m\u001b[0;32m   1200\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: When using data tensors as input to a model, you should specify the `steps_per_epoch` argument."
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=40) #, steps_per_epoch = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.save('../model/OCR_ver1.0.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4. 모델 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 55, 55, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 51, 51, 3)         78        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 25, 25, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 21, 21, 6)         456       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 10, 6)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 84)                50484     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 122)               10370     \n",
      "=================================================================\n",
      "Total params: 61,388\n",
      "Trainable params: 61,388\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
